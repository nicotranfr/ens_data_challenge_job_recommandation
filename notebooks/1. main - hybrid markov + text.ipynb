{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30748e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_MIN_DF = 3\n",
    "TFIDF_MAX_FEATURES = None\n",
    "NGRAM_RANGE = (1, 2)\n",
    "TEXT_TOP_N = 50\n",
    "TOP_K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155713a6",
   "metadata": {},
   "source": [
    "2 problems: \n",
    "\n",
    "#### I. Job prediction \n",
    "\n",
    "- Hybrid recommender system combining sequential and textual signals:\n",
    "1. Markov chain to model job-to-job transitions\n",
    "2. TF-IDF + cosine similarity for cold start and content fallback\n",
    "Final ranking based on transition probability boosted by text similarity\n",
    "\n",
    "#### II. Action Prediction\n",
    "- Logistic Regression classifier, predicts next user action (e.g., apply vs view)\n",
    "- Uses trailing number of recent \"view\" interactions as input feature, e.g. captures short-term engagement intensity signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d30ed",
   "metadata": {},
   "source": [
    "# I. Job Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a236b",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = Path(\"/Users/enfants/Code/Job Offer Recommendation\")\n",
    "data_dir = repo_dir / \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(data_dir / \"x_train.csv\")\n",
    "y_train = pd.read_csv(data_dir / \"y_train.csv\")\n",
    "X_test  = pd.read_csv(data_dir / \"x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jobs_json(path):\n",
    "    \"\"\"\n",
    "    Loads job_listings.json structured as:\n",
    "    {\n",
    "        \"0\": \"TEXT...\",\n",
    "        \"1\": \"TEXT...\",\n",
    "        ...\n",
    "    }\n",
    "    Returns a DataFrame with columns:\n",
    "    - job_id (int)\n",
    "    - job_text_raw (str)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(\"Expected JSON to be a dict of {job_id: text}\")\n",
    "\n",
    "    jobs_df = pd.DataFrame({\n",
    "        \"job_id\": list(data.keys()),\n",
    "        \"job_text_raw\": list(data.values())\n",
    "    })\n",
    "\n",
    "    jobs_df[\"job_id\"] = jobs_df[\"job_id\"].astype(int)\n",
    "\n",
    "    return jobs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb543e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = load_jobs_json(data_dir / \"job_listings.json\")\n",
    "\n",
    "print(jobs_df.shape)\n",
    "jobs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd5073",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb1a60",
   "metadata": {},
   "source": [
    "### 2.1 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f36cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Basic cleaning: lowercase, remove weird chars, collapse spaces.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                 # collapse whitespace\n",
    "    text = re.sub(r\"[^\\w\\s\\-\\/\\+]\", \" \", text)       # keep letters/digits/_ and a few separators\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df[\"job_text\"] = jobs_df[\"job_text_raw\"].map(clean_text)\n",
    "jobs_df = jobs_df[[\"job_id\", \"job_text\"]]\n",
    "\n",
    "print(jobs_df.shape)\n",
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8672d9f",
   "metadata": {},
   "source": [
    "### 2.2 Job Sequence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_column(df, col):\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "# parse input data (train & test)\n",
    "X_train = parse_list_column(X_train, \"job_ids\")\n",
    "X_train = parse_list_column(X_train, \"actions\")\n",
    "\n",
    "X_test  = parse_list_column(X_test, \"job_ids\")\n",
    "X_test  = parse_list_column(X_test, \"actions\")\n",
    "\n",
    "\n",
    "# merge train inputs with targets\n",
    "df = X_train.merge(y_train, on=\"session_id\")\n",
    "\n",
    "\n",
    "# rebuild full sequences (jobs + actions)\n",
    "df[\"full_job_sequence\"] = (\n",
    "    df[\"job_ids\"] + df[\"job_id\"].apply(lambda x: [x])\n",
    ")\n",
    "\n",
    "df[\"full_action_sequence\"] = (\n",
    "    df[\"actions\"] + df[\"action\"].apply(lambda x: [x])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2493518",
   "metadata": {},
   "source": [
    "## 3. Evaluation: MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "\n",
    "train_sessions, val_sessions = train_test_split(\n",
    "    df[\"session_id\"].unique(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df[\"session_id\"].isin(train_sessions)]\n",
    "val_df = df[df[\"session_id\"].isin(val_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4845ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement metrics : Mean Reciprocal Ranking\n",
    "\n",
    "def compute_mrr(y_true, y_pred, k=10):\n",
    "    \"\"\"\n",
    "    y_true: list of true job_ids\n",
    "    y_pred: list of list, each inner list = top-k predicted job_ids\n",
    "    \"\"\"\n",
    "    rr = []\n",
    "    for true_job, preds in zip(y_true, y_pred):\n",
    "        if true_job in preds[:k]:\n",
    "            rank = preds.index(true_job) + 1\n",
    "            rr.append(1 / rank)\n",
    "        else:\n",
    "            rr.append(0)\n",
    "    return sum(rr) / len(rr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658789af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk(y_true, y_pred, k=10):\n",
    "    \"\"\"\n",
    "    y_true : list[int]\n",
    "    y_pred : list[list[int]]\n",
    "    k : int (top-k)\n",
    "\n",
    "    Returns a dict with:\n",
    "        - miss_rate\n",
    "        - mean_rank (when hit)\n",
    "        - min_rank\n",
    "        - max_rank\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred), \"y_true and y_pred must have same length\"\n",
    "\n",
    "    hits = []\n",
    "    ranks = []\n",
    "\n",
    "    for true_job, preds in zip(y_true, y_pred):\n",
    "        top_preds = preds[:k]\n",
    "\n",
    "        if true_job in top_preds:\n",
    "            hits.append(1)\n",
    "            ranks.append(top_preds.index(true_job) + 1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "    miss_rate = 1 - (sum(hits) / len(hits))\n",
    "\n",
    "    if ranks:\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "        min_rank = min(ranks)\n",
    "        max_rank = max(ranks)\n",
    "    else:\n",
    "        mean_rank = None\n",
    "        min_rank = None\n",
    "        max_rank = None\n",
    "\n",
    "    return {\n",
    "        \"miss_rate\": miss_rate,\n",
    "        \"mean_rank_when_hit\": mean_rank,\n",
    "        \"min_rank\": min_rank,\n",
    "        \"max_rank\": max_rank\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee053d40",
   "metadata": {},
   "source": [
    "## 4. Text Model (TF-IDF → vector by job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf(\n",
    "    jobs_df: pd.DataFrame,\n",
    "    min_df: int = TFIDF_MIN_DF,\n",
    "    max_features: int = TFIDF_MAX_FEATURES,\n",
    "    ngram_range: Tuple[int, int] = NGRAM_RANGE,\n",
    ") -> Tuple[TfidfVectorizer, np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    Term Frequency – Inverse Document Frequency. Converts text into numerical vectors, giving more \n",
    "    weight to words that are frequent in a document but rare in the entire corpus.\n",
    "    \n",
    "    Returns:\n",
    "      - fitted vectorizer\n",
    "      - tfidf_matrix (sparse)\n",
    "      - job_ids aligned with matrix rows\n",
    "    \"\"\"\n",
    "    job_ids = jobs_df[\"job_id\"].astype(int).tolist()\n",
    "    texts = jobs_df[\"job_text\"].fillna(\"\").tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        strip_accents=\"unicode\",\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix, job_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddaf464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_neighbors(\n",
    "    tfidf_matrix,\n",
    "    job_ids: List[int],\n",
    "    top_n: int = 50,\n",
    "    n_jobs: int = -1\n",
    ") -> Dict[int, List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    Returns mapping:\n",
    "      job_id -> [(neighbor_job_id, similarity), ...] sorted by similarity desc\n",
    "    \"\"\"\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=top_n + 1,  # +1 because the closest is itself\n",
    "        metric=\"cosine\",\n",
    "        algorithm=\"brute\",\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    nn.fit(tfidf_matrix)\n",
    "\n",
    "    distances, indices = nn.kneighbors(tfidf_matrix, return_distance=True)\n",
    "\n",
    "    id_by_row = np.array(job_ids)\n",
    "    neighbors_map: Dict[int, List[Tuple[int, float]]] = {}\n",
    "\n",
    "    for row_idx in range(indices.shape[0]):\n",
    "        src_job_id = int(id_by_row[row_idx])\n",
    "        row_neighbors = []\n",
    "        for d, j in zip(distances[row_idx], indices[row_idx]):\n",
    "            neighbor_job_id = int(id_by_row[j])\n",
    "            if neighbor_job_id == src_job_id:\n",
    "                continue\n",
    "            sim = float(1.0 - d)  # cosine distance -> similarity\n",
    "            row_neighbors.append((neighbor_job_id, sim))\n",
    "        neighbors_map[src_job_id] = row_neighbors[:top_n]\n",
    "\n",
    "    return neighbors_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, tfidf_matrix, tfidf_job_ids = build_tfidf(jobs_df)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff75724",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_neighbors = build_text_neighbors(tfidf_matrix, tfidf_job_ids, top_n=TEXT_TOP_N)\n",
    "\n",
    "text_neighbors[100][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0215d1",
   "metadata": {},
   "source": [
    "## 5. Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6903b4",
   "metadata": {},
   "source": [
    "### 5.1 Build Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all transitions (job_i to job_i+1)\n",
    "\n",
    "transition_counts = defaultdict(int)\n",
    "\n",
    "for seq in df[\"full_job_sequence\"]:\n",
    "    for i in range(len(seq) - 1):\n",
    "        transition_counts[(seq[i], seq[i+1])] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transition_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build job-to-job matrix\n",
    "\n",
    "trans_df = pd.DataFrame(\n",
    "    [(i, j, c) for (i, j), c in transition_counts.items()],\n",
    "    columns=[\"from_job\", \"to_job\", \"count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df[\"count\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Transition count\")\n",
    "plt.ylabel(\"Number of job-to-job pairs\")\n",
    "plt.title(\"Distribution of job-to-job transition counts\")\n",
    "plt.tight_layout()\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize in conditional probabilities\n",
    "# Knowing that the candidate is in job A, what is the probability that they will move to job B?\n",
    "\n",
    "trans_df[\"prob\"] = (\n",
    "    trans_df[\"count\"] /\n",
    "    trans_df.groupby(\"from_job\")[\"count\"].transform(\"sum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc62b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c182ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.groupby(\"from_job\")[\"prob\"].sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36349f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_markov_map(trans_df: pd.DataFrame, top_n: int = TEXT_TOP_N) -> Dict[int, List[Tuple[int, float]]]:\n",
    "    trans_df_sorted = trans_df.sort_values([\"from_job\", \"prob\"], ascending=[True, False])\n",
    "    markov_map = {}\n",
    "    for from_job, group in trans_df_sorted.groupby(\"from_job\"):\n",
    "        top = group.head(top_n)[[\"to_job\", \"prob\"]].values.tolist()\n",
    "        markov_map[int(from_job)] = [(int(j), float(p)) for j, p in top]\n",
    "    return markov_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcace3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fallback in case job_id is not in from_job : overall popularity of jobs\n",
    "\n",
    "popular_jobs = (\n",
    "    trans_df.groupby(\"to_job\")[\"count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    "    .index\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement top 10 from last job\n",
    "# \"If the last job viewed is X, recommend the 10 most likely jobs after X.\"\n",
    "\n",
    "def recommend_next_jobs(last_job, trans_df, popular_jobs, k=10):\n",
    "    candidates = trans_df[trans_df[\"from_job\"] == last_job]\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return popular_jobs[:k]\n",
    "    \n",
    "    return (\n",
    "        candidates\n",
    "        .sort_values(\"prob\", ascending=False)\n",
    "        .head(k)[\"to_job\"]\n",
    "        .tolist()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_job = trans_df[\"from_job\"].iloc[0]\n",
    "recommend_next_jobs(example_job, trans_df, popular_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jobs texte:\", jobs_df[\"job_id\"].nunique())\n",
    "print(\"Jobs Markov from_job:\", trans_df[\"from_job\"].nunique())\n",
    "print(\"Jobs Markov to_job:\", trans_df[\"to_job\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_job_popularity(trans_df: pd.DataFrame, seq_col: str = \"full_job_sequence\") -> Dict[int, float]:\n",
    "    counts = {}\n",
    "    for seq in trans_df[seq_col]:\n",
    "        for job_id in seq:\n",
    "            counts[job_id] = counts.get(job_id, 0) + 1\n",
    "\n",
    "    total = sum(counts.values()) if counts else 1\n",
    "    popularity = {job_id: c / total for job_id, c in counts.items()}\n",
    "    return popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_popularity = build_job_popularity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc4b42",
   "metadata": {},
   "source": [
    "### 5.2 Session-aware recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54274533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group all recommendations on the whole session\n",
    "\n",
    "# create a dictionnary from_job -> list[(to_job, prob)]\n",
    "\n",
    "next_map = defaultdict(list)\n",
    "for r in trans_df.itertuples(index=False):\n",
    "    next_map[r.from_job].append((r.to_job, r.prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b84fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_from_session(job_seq, next_map, popular_jobs, k=10):\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    for j in job_seq:\n",
    "        for to_job, prob in next_map.get(j, []):\n",
    "            scores[to_job] += prob\n",
    "\n",
    "    # fallback if no signal\n",
    "    if not scores:\n",
    "        return popular_jobs[:k]\n",
    "\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [job for job, _ in ranked[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add weighted score by recency\n",
    "\n",
    "def recommend_from_session_recency(job_seq, next_map, popular_jobs, k=10):\n",
    "    scores = defaultdict(float)\n",
    "    L = len(job_seq)\n",
    "\n",
    "    for i, j in enumerate(job_seq):\n",
    "        weight = (i + 1) / L   # linear recency\n",
    "        for to_job, prob in next_map.get(j, []):\n",
    "            scores[to_job] += weight * prob\n",
    "\n",
    "    if not scores:\n",
    "        return popular_jobs[:k]\n",
    "\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [job for job, _ in ranked[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94238c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example on one session\n",
    "\n",
    "seq = val_df.iloc[2][\"full_job_sequence\"]\n",
    "history = seq[:-1]        # everything except the target\n",
    "true_job = seq[-1]\n",
    "\n",
    "preds_plain = recommend_from_session(history, next_map, popular_jobs)\n",
    "preds_recent = recommend_from_session_recency(history, next_map, popular_jobs)\n",
    "\n",
    "print(\"History:\", history)\n",
    "print(\"True job:\", true_job)\n",
    "print(\"Plain:\", preds_plain)\n",
    "print(\"Rank plain:\", preds_plain.index(true_job) + 1 if true_job in preds_plain else \"MISS\")\n",
    "print(\"Recency:\", preds_recent)\n",
    "print(\"Rank recency:\", preds_recent.index(true_job) + 1 if true_job in preds_recent else \"MISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb71c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test differents k-values\n",
    "\n",
    "k_values = [1, 3, 5, 7, 10]\n",
    "\n",
    "def predict_last_k(seq, k):\n",
    "    return seq[-k:] if len(seq) >= k else seq\n",
    "\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in val_df.iterrows():\n",
    "        seq = row[\"full_job_sequence\"]\n",
    "        history = predict_last_k(seq[:-1], k)\n",
    "        true_job = seq[-1]\n",
    "\n",
    "        preds = recommend_from_session_recency(\n",
    "            history,\n",
    "            next_map,\n",
    "            popular_jobs,\n",
    "            k=10\n",
    "        )\n",
    "\n",
    "        y_true.append(true_job)\n",
    "        y_pred.append(preds)\n",
    "\n",
    "    mrr = compute_mrr(y_true, y_pred, k=10)\n",
    "    results[k] = mrr\n",
    "    print(f\"k={k} → MRR@10 = {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8b7ec",
   "metadata": {},
   "source": [
    "Performance decreases as k last jobs increases.\n",
    "The best MRR@10 is obtained with **k = 1**, meaning the last visited job is the most informative.\n",
    "\n",
    "\n",
    "This confirms that user intent shifts quickly and that older interactions add noise rather than signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebfacf",
   "metadata": {},
   "source": [
    "## 6. Hybrid Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_jobs(session_job_ids, k=10):\n",
    "    seen = set(session_job_ids)\n",
    "    last_job = session_job_ids[-1] if session_job_ids else None\n",
    "    candidates = {}\n",
    "\n",
    "    # 1. Markov\n",
    "    if last_job in markov_map:\n",
    "        for j, p in markov_map[last_job]:\n",
    "            if j not in seen:\n",
    "                candidates[j] = p\n",
    "\n",
    "    # 2. Texte if few candidates\n",
    "    if len(candidates) < k and last_job in text_neighbors:\n",
    "        for j, sim in text_neighbors[last_job]:\n",
    "            if j not in seen:\n",
    "                candidates[j] = candidates.get(j, 0) + 0.8 * sim\n",
    "\n",
    "    # 3. Popularity fallback\n",
    "    if len(candidates) < k:\n",
    "        for j, pop in sorted(job_popularity.items(), key=lambda x: x[1], reverse=True):\n",
    "            if j not in seen:\n",
    "                candidates[j] = candidates.get(j, 0) + 0.1 * pop\n",
    "            if len(candidates) >= 200:\n",
    "                break\n",
    "\n",
    "    ranked = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [j for j, _ in ranked[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_markov_only(session_job_ids: List[int], markov_map, popularity, k=10) -> List[int]:\n",
    "    last_job = session_job_ids[-1] if session_job_ids else None\n",
    "    seen = set(session_job_ids)\n",
    "\n",
    "    candidates = {}\n",
    "    if last_job is not None and last_job in markov_map:\n",
    "        for j, p in markov_map[last_job]:\n",
    "            if j not in seen:\n",
    "                candidates[j] = max(candidates.get(j, 0.0), p)\n",
    "\n",
    "    # fallback popularity\n",
    "    if len(candidates) < k and popularity:\n",
    "        for j, pop in sorted(popularity.items(), key=lambda x: x[1], reverse=True):\n",
    "            if j in seen:\n",
    "                continue\n",
    "            candidates[j] = candidates.get(j, 0.0) + 0.01 * pop\n",
    "            if len(candidates) >= 500:\n",
    "                break\n",
    "\n",
    "    ranked = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [j for j, _ in ranked[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c71f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_text_only(session_job_ids, k=10):\n",
    "    last_job = session_job_ids[-1]\n",
    "    seen = set(session_job_ids)\n",
    "    return [\n",
    "        j for j, _ in text_neighbors.get(last_job, [])\n",
    "        if j not in seen\n",
    "    ][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_recommend(job_ids, k=10):\n",
    "    recs = recommend_jobs(job_ids, k=k)\n",
    "\n",
    "    if len(recs) < k:\n",
    "        seen = set(job_ids) | set(recs)\n",
    "        for j, _ in sorted(job_popularity.items(), key=lambda x: x[1], reverse=True):\n",
    "            if j not in seen:\n",
    "                recs.append(j)\n",
    "            if len(recs) == k:\n",
    "                break\n",
    "\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cbad7",
   "metadata": {},
   "source": [
    "## 7. Offline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MRR@10 on validation\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _, row in val_df.iterrows():\n",
    "    last_job = row[\"full_job_sequence\"][-2]          # last job seen\n",
    "    true_job = row[\"full_job_sequence\"][-1]              # target jon\n",
    "    \n",
    "    preds = recommend_next_jobs(\n",
    "        last_job,\n",
    "        trans_df,\n",
    "        popular_jobs,\n",
    "        k=10\n",
    "    )\n",
    "    \n",
    "    y_true.append(true_job)\n",
    "    y_pred.append(preds)\n",
    "\n",
    "# saving baseline score\n",
    "\n",
    "mrr = compute_mrr(y_true, y_pred, k=10)\n",
    "print(f\"MRR@10 (baseline Markov): {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_topk(y_true, y_pred, k=10)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ae564",
   "metadata": {},
   "source": [
    "## 8. Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, tfidf_matrix, tfidf_job_ids = build_tfidf(\n",
    "    jobs_df,\n",
    "    min_df=TFIDF_MIN_DF,\n",
    "    max_features=None,\n",
    "    ngram_range=NGRAM_RANGE\n",
    ")\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_neighbors = build_text_neighbors(\n",
    "    tfidf_matrix,\n",
    "    tfidf_job_ids,\n",
    "    top_n=TEXT_TOP_N\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_map = build_markov_map(trans_df, top_n=TEXT_TOP_N)\n",
    "job_popularity = build_job_popularity(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = [recommend_text_only(seq, k=10) for seq in val_df[\"job_ids\"]]\n",
    "val_true = val_df[\"job_id\"].tolist()\n",
    "\n",
    "mrr_hybrid = compute_mrr(val_true, val_pred, k=10)\n",
    "print(\"MRR Text Only:\", mrr_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f82102",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_markov = [recommend_markov_only(seq, markov_map, job_popularity, k=10) for seq in val_df[\"job_ids\"]]\n",
    "mrr_markov = compute_mrr(val_true, val_pred_markov, k=10)\n",
    "\n",
    "print(\"MRR@10 markov:\", mrr_markov)\n",
    "print(\"Delta:\", mrr_hybrid - mrr_markov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66467cc",
   "metadata": {},
   "source": [
    "# II. Action Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8f748",
   "metadata": {},
   "source": [
    "### 1. Small EDA on number of 'views'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect initial distribution\n",
    "\n",
    "df[\"action\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_view\"] = df[\"actions\"].apply(\n",
    "    lambda x: len(ast.literal_eval(x)) if isinstance(x, str) else len(x)\n",
    ")\n",
    "\n",
    "prop = (\n",
    "    df.groupby(\"n_view\")[\"action\"]\n",
    "      .value_counts(normalize=True)\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "prop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of previous views (n_view)\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Probability of next action given nb. of views in the series\")\n",
    "plt.legend(title=\"Next action\")\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trailing_views(actions):\n",
    "    c = 0\n",
    "    for a in reversed(actions):\n",
    "        if a == \"view\":\n",
    "            c += 1\n",
    "        else:\n",
    "            break\n",
    "    return c\n",
    "\n",
    "df[\"trail_view\"] = df[\"actions\"].apply(\n",
    "    lambda x: trailing_views(ast.literal_eval(x)) if isinstance(x, str) else trailing_views(x)\n",
    ")\n",
    "\n",
    "\n",
    "prop2 = (\n",
    "    df.groupby(\"trail_view\")[\"action\"]\n",
    "      .value_counts(normalize=True)\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "prop2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d922e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop2.plot(kind=\"bar\", stacked=True)\n",
    "\n",
    "plt.xlabel(\"Number of consecutive views at end of sequence\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Next action distribution vs trailing nb. of views\")\n",
    "plt.legend(title=\"Next action\")\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b259c8a",
   "metadata": {},
   "source": [
    "### 2. Linear regression using one feature: the number of trailing 'view' actions before the next action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ab0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "\n",
    "action_df = df[[\"full_action_sequence\"]].copy()\n",
    "\n",
    "# history = all actions except the last one\n",
    "action_df[\"action_history\"] = action_df[\"full_action_sequence\"].apply(lambda x: x[:-1])\n",
    "\n",
    "# target = last action\n",
    "action_df[\"action_target\"] = action_df[\"full_action_sequence\"].apply(lambda x: x[-1])\n",
    "\n",
    "\n",
    "# feature engineering\n",
    "\n",
    "def trailing_views(actions):\n",
    "    c = 0\n",
    "    for a in reversed(actions):\n",
    "        if a == \"view\":\n",
    "            c += 1\n",
    "        else:\n",
    "            break\n",
    "    return c\n",
    "\n",
    "action_df[\"trail_view\"] = action_df[\"action_history\"].apply(trailing_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "\n",
    "X_action = action_df[[\"trail_view\"]]\n",
    "y_action = action_df[\"action_target\"]\n",
    "\n",
    "action_clf = LogisticRegression()\n",
    "action_clf.fit(X_action, y_action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b47b82",
   "metadata": {},
   "source": [
    "y_pred = action_clf.predict(X_val_ml)\n",
    "\n",
    "print(classification_report(y_val_ml, y_pred))\n",
    "\n",
    "bal_acc = balanced_accuracy_score(y_val_ml, y_pred)\n",
    "print(f\"Balanced accuracy: {bal_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135bf46",
   "metadata": {},
   "source": [
    "- We train a simple logistic regression using one feature: the number of trailing 'view' actions.\n",
    "- This feature is highly predictive of the next action.\n",
    "- The model achieves 83% accuracy, far above the 60% baseline.\n",
    "- It detects most 'apply' actions, with a recall of 87%.\n",
    "- The more consecutive 'view' actions at the end of a session, the more likely the next action is to be 'apply'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ad54b",
   "metadata": {},
   "source": [
    "### 3. predict_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(actions):\n",
    "    \"\"\"\n",
    "    actions: list[str] (history only)\n",
    "    returns: \"view\" or \"apply\"\n",
    "    \"\"\"\n",
    "    tv = trailing_views(actions)\n",
    "    X = pd.DataFrame([[tv]], columns=[\"trail_view\"])\n",
    "    return action_clf.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80dee0",
   "metadata": {},
   "source": [
    "# Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f17acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_session(job_ids, actions):\n",
    "    \"\"\"\n",
    "    job_ids : list[int]\n",
    "    actions : list[str]\n",
    "    returns : (top10_jobs: list[int], action: str)\n",
    "    \"\"\"\n",
    "\n",
    "    # Hybrid job recommendation\n",
    "    top10_jobs = safe_recommend(job_ids, k=10)\n",
    "\n",
    "    # Action prediction\n",
    "    action = predict_action(actions)\n",
    "\n",
    "    return top10_jobs, action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for _, row in X_test.iterrows():\n",
    "    top10_jobs, action = predict_session(\n",
    "        row[\"job_ids\"],\n",
    "        row[\"actions\"]\n",
    "    )\n",
    "\n",
    "    predictions.append({\n",
    "        \"session_id\": row[\"session_id\"],\n",
    "        \"action\": action,\n",
    "        \"job_id\": top10_jobs\n",
    "    })\n",
    "\n",
    "submission_df = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e43007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.shape)\n",
    "\n",
    "# 10 jobs par ligne\n",
    "print(submission_df[\"job_id\"].apply(len).value_counts())\n",
    "\n",
    "# distribution action\n",
    "print(submission_df[\"action\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_jobs = set(trans_df[\"from_job\"].unique())\n",
    "\n",
    "cold_rate = sum(\n",
    "    row[\"job_ids\"][-1] not in from_jobs\n",
    "    for _, row in X_test.iterrows()\n",
    ") / len(X_test)\n",
    "\n",
    "cold_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e671e",
   "metadata": {},
   "source": [
    "## Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dir = repo_dir / \"output\"\n",
    "submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find existing versions\n",
    "existing_files = list(submission_dir.glob(\"submission_hybrid_v*.csv\"))\n",
    "\n",
    "version_numbers = []\n",
    "\n",
    "for f in existing_files:\n",
    "    match = re.search(r\"v(\\d+)\", f.stem)\n",
    "    if match:\n",
    "        version_numbers.append(int(match.group(1)))\n",
    "\n",
    "next_version = max(version_numbers, default=0) + 1\n",
    "\n",
    "file_name = f\"submission_hybrid_v{next_version}.csv\"\n",
    "file_path = submission_dir / file_name\n",
    "\n",
    "submission_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384687aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Shape:\", check_df.shape)\n",
    "print(check_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d827b6",
   "metadata": {},
   "source": [
    "# Run Whole Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14023ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "\n",
    "    print(\"Building text model...\")\n",
    "    vectorizer, tfidf_matrix, tfidf_job_ids = build_tfidf(\n",
    "        jobs_df,\n",
    "        min_df=TFIDF_MIN_DF,\n",
    "        max_features=TFIDF_MAX_FEATURES,\n",
    "        ngram_range=NGRAM_RANGE\n",
    "    )\n",
    "\n",
    "    text_neighbors = build_text_neighbors(\n",
    "        tfidf_matrix,\n",
    "        tfidf_job_ids,\n",
    "        top_n=TEXT_TOP_N\n",
    "    )\n",
    "\n",
    "    print(\"Building Markov model...\")\n",
    "    markov_map = build_markov_map(trans_df, top_n=TEXT_TOP_N)\n",
    "    popularity = build_job_popularity(train_df)\n",
    "\n",
    "    print(\"Generating submission...\")\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in X_test.iterrows():\n",
    "        top10_jobs = recommend_jobs(row[\"job_ids\"])\n",
    "\n",
    "        action = predict_action(row[\"actions\"])\n",
    "\n",
    "        predictions.append({\n",
    "            \"session_id\": row[\"session_id\"],\n",
    "            \"action\": action,\n",
    "            \"job_id\": top10_jobs\n",
    "        })\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions)\n",
    "\n",
    "    print(\"Sanity checks...\")\n",
    "    assert submission_df.shape[0] == len(X_test)\n",
    "    assert submission_df[\"job_id\"].apply(len).eq(TOP_K).all()\n",
    "    assert submission_df.isna().sum().sum() == 0\n",
    "\n",
    "    print(\"Saving submission...\")\n",
    "\n",
    "    submission_dir = repo_dir / \"output\"\n",
    "    submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Find existing versions\n",
    "    existing_files = list(submission_dir.glob(\"submission_hybrid_v*.csv\"))\n",
    "\n",
    "    version_numbers = []\n",
    "\n",
    "    for f in existing_files:\n",
    "        match = re.search(r\"v(\\d+)\", f.stem)\n",
    "        if match:\n",
    "            version_numbers.append(int(match.group(1)))\n",
    "\n",
    "    next_version = max(version_numbers, default=0) + 1\n",
    "\n",
    "    file_name = f\"submission_hybrid_v{next_version}.csv\"\n",
    "    file_path = submission_dir / file_name\n",
    "\n",
    "    submission_df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Saved to: {file_path}\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd094ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (churn)",
   "language": "python",
   "name": "churn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
